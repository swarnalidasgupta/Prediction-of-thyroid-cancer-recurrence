---
title: "Thyroid_cancer_prediction"
author: "Swarnali Dasgupta"
date: "2024-09-25"
output: pdf_document
---

## This project aims to train machine learning models to predict the likelihood of thyroid cancer recurrence in patients with well differentiated thyroid cancer. Each row in the data represents a patient. Random forest model, KNN based model, SVM model and Artificial neural networks were trained to predict the possibility of recurrence and all the models were compared to determine the most efficacious model. 

#### Load all required libraries
```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(archive)
library(caret)
library(randomForest)
library(nnet)
library(gmodels)
library(class)
```

#### Read in the pre-processed dataset
```{r}
url <- ("https://archive.ics.uci.edu/static/public/915/differentiated+thyroid+cancer+recurrence.zip")
all_data <- read_csv(archive_read(url), show_col_types = FALSE)
data <- all_data
```


### Data preparation

#### Explore the data and check for missing values
```{r}
#str(data)
#dim(data)
summary(data)
ifelse(any(is.na(data)), "NA values present", "No NA values")
```
There are 383 patients with 17 clinico-pathological features. 

#### Detect outliers
```{r}
colnames <- colnames(data)
outlier_detect <- function(data,colname){
  mean <- mean(data[[colname]])
  sd <- sd(data[[colname]])
  zscore <- (data[[colname]] - mean) / sd
}

for (colname in colnames){
  data <- outlier_detect(data, colname)
}
```



#### One-hot encoding to make the columns readable- for Yes and No columns
```{r}
#Longer technique
#data$Gender <-ifelse(data$Gender == "M", gsub("M", 1, data$Gender), gsub("F", 0, data$Gender))
#data$Smoking <-ifelse(data$Smoking == "Yes", gsub("Yes", 1, data$Smoking), gsub("No", 0, data$Smoking))

#Create a mapping function to automate
mappings <- list(
  Gender = c("M" = 1, "F" = 0),
  Smoking = c("Yes" = 1, "No" = 0),
  'Hx Smoking' = c("Yes" = 1, "No" = 0),
  'Hx Radiothreapy' = c("Yes" = 1, "No" = 0),
  Recurred = c("Yes" = 1, "No" = 0)
)

binary_encoding <- function(data, col.name, mappings) {
  if (col.name %in% names(mappings)) {
    data[[col.name]] <- mappings[[col.name]][data[[col.name]]]
  }
  return(data)  
}

for (colname in colnames(data)) {
  data <- binary_encoding(data, colname, mappings)
}
```

#### One-hot encoding to make columns readable- for categorical columns
```{r}
categorical_columns <- c("Adenopathy", "Thyroid Function", "Physical Examination", "Pathology", "Focality", "Risk", "T", "N", "M", "Stage", "Response")
categorical_df <- data[, categorical_columns]
data <- data[, -c(6:7, 8:16)]

#Use caret package to create one hot encoding
dummies <- dummyVars(" ~ .", data = categorical_df) 
one_hot_encoded <- predict(dummies, newdata = categorical_df)
one_hot_encoded <- as.data.frame(one_hot_encoded)

#cbind to original dataframe
data <- cbind(data, one_hot_encoded)

#Make syntactically valid column names
colnames(data) <- make.names(colnames(data))

```

##### Normalize the age column using min-max scaling
```{r}
data$Scaled.Age <- (data$Age - min(data$Age))/(max(data$Age) - min(data$Age))
data <- data[, -1]
```

#### Clean up column names to make processing easier
```{r}
#Remove spaces in the column names using gsub
colnames(data) <- gsub("^X.", "", colnames(data))
#colnames

#colnames(train.data) <- gsub(" ", ".", colnames(train.data))
#colnames(train.data) <- gsub("`", "", colnames(train.data))
#colnames(train.data) <- gsub("-", ".", colnames(train.data))
```

### Splitting the data and training the models
After processing the data, we will split the data into training and testing sets. We will use 70% patients for training and 30%  patients for testing. We will use the recurred column for splitting.


#### Logistic regression model
```{r, warning = FALSE}
#Create data split 
train.index <- createDataPartition(data$Recurred, p = 0.7, list = FALSE)
train.data <- data[train.index,]
test.data <- data[-train.index,]

#Check proportion of data in original vs split datasets
prop.table(table(data$Recurred)) * 100
prop.table(table(train.data$Recurred)) * 100
prop.table(table(test.data$Recurred)) * 100
#Generate labels for the knn function
train.labels <- train.data$Recurred
test.labels <- test.data$Recurred

#Replace 0 with "Not recurred" and 1 with "Recurred" for better clarity
train.labels <- factor(train.labels, levels = c(0, 1), labels = c("Not recurred", "Recurred"))
test.labels <- factor(test.labels, levels = c(0, 1), labels = c("Not recurred", "Recurred"))

#Relevel the Recurred column
train.labels <- factor(train.labels, levels = c("Recurred", "Not recurred"))
test.labels <- factor(test.labels, levels = c("Recurred", "Not recurred"))

#Remove the recurred column from the test and train sets
train.data <- train.data[, -which(colnames(train.data) == "Recurred")]
test.data <- test.data[, -which(colnames(test.data) == "Recurred")]

########Data is imbalanced, so assigning weights#######
total.train.samples <- nrow(train.data)
weight.recurred <- total.train.samples / (2 * sum(train.labels == "Recurred"))
weight.not.recurred <- total.train.samples / (2 * sum(train.labels == "Not recurred"))

#Assign weights to each sample in the training set
class.weights <- ifelse(train.labels == "Recurred", weight.recurred, weight.not.recurred)

#Fit the logistic regression model with weights
logistic.model <- glm(train.labels ~ ., data = train.data, family = binomial, weights = class.weights)

#Summarize the model
summary(logistic.model)

#Predict using the model on test data
logistic.prediction <- predict(logistic.model, newdata = test.data, type = "response")

#Convert probabilities to binary (0/1) using a threshold of 0.5
logistic.prediction.class <- ifelse(logistic.prediction > 0.5, "Recurred", "Not recurred")

#Convert the predictions to a factor with the same levels as the actual labels
logistic.prediction.class <- factor(logistic.prediction.class, levels = levels(test.labels))

#Create confusion matrix (compare predicted classes to true test labels)
confusionMatrix(logistic.prediction.class, test.labels)
```

The logistic regression model has a very low accuracy rate, so we will a technique called SMOTE to rectify the class imabalance in the data before moving on with other modelling techniques.


#### Random forest model
```{r}
#Create data split
train.index <- createDataPartition(data$Recurred, p = 0.7, list = FALSE)
train.data <- data[train.index,]
test.data <- data[-train.index,]

#Generate labels for the knn function
train.labels <- train.data$Recurred
test.labels <- test.data$Recurred

#Replace 0 with "Not recurred" and 1 with "Recurred" for better clarity
train.labels <- factor(train.labels, levels = c(0, 1), labels = c("Not recurred", "Recurred"))
test.labels <- factor(test.labels, levels = c(0, 1), labels = c("Not recurred", "Recurred"))

#Relevel the Recurred column
train.labels <- factor(train.labels, levels = c("Recurred", "Not recurred"))
test.labels <- factor(test.labels, levels = c("Recurred", "Not recurred"))

#Remove the recurred column from the test and train sets
train.data <- train.data[, -which(colnames(train.data) == "Recurred")]
test.data <- test.data[, -which(colnames(test.data) == "Recurred")]

#Fit the random forest model
rf.model <- randomForest(x = train.data, y = train.labels, ntree = 100, importance = TRUE)
print(rf.model)

#Predict test data using the model
rf.predictions <- predict(rf.model, newdata = test.data)

#Create a confusion matrix
confusionMatrix(rf.predictions, test.labels)
```

#### KNN model
```{r}
#Create data split
train.index <- createDataPartition(data$Recurred, p = 0.7, list = FALSE)
train.data <- data[train.index,]
test.data <- data[-train.index,]

#Generate labels for the knn function
train.labels <- train.data$Recurred
test.labels <- test.data$Recurred

#Replace 0 with "Not recurred" and 1 with "Recurred" for better clarity
train.labels <- factor(train.labels, levels = c(0, 1), labels = c("Not recurred", "Recurred"))
test.labels <- factor(test.labels, levels = c(0, 1), labels = c("Not recurred", "Recurred"))

#Relevel the Recurred column
train.labels <- factor(train.labels, levels = c("Recurred", "Not recurred"))
test.labels <- factor(test.labels, levels = c("Recurred", "Not recurred"))

#Remove the recurred column from the test and train sets
train.data <- train.data[, -which(colnames(train.data) == "Recurred")]
test.data <- test.data[, -which(colnames(test.data) == "Recurred")]

#Check the structure of both train and test sets
#str(train.data)
#str(test.data)

#Estimate k as square root of number of observations in train set
k = round(sqrt(nrow(train.data)))
#Build the model
knn.model <- knn(train = train.data, test = test.data, cl = train.labels, k = k)
summary(knn.model)

#Create a confusion matrix
confusionMatrix(knn.model, as.factor(test.labels))

#Create a CrossTable
CrossTable(x = test.labels, y = knn.model,
             prop.chisq = FALSE)
```

#### SVM model





